[["index.html", "S1Z Lab 4 1 Welcome to S1Z Lab 4 1.1 Intended Learning Outcomes", " S1Z Lab 4 1 Welcome to S1Z Lab 4 1.1 Intended Learning Outcomes In this lab we explore multiple regression, which introduces the possibility of more than one predictor variable in a linear model. This builds on Lab 3 by adding in more than one predictor in the model. This lab covers material in OpenIntro Chapter 9. Feel free to refer back to the material to help you within this lab. We will explore and visualise the data using the tidyverse suite of packages and analyse data contained in the openintro package. We will use the GGally package to access the function ggpairs which produces a plot for each variable against each other displayed on one plot. You'll need to ensure you have the following packages installed and loaded: install.packages(&quot;tidyverse&quot;) install.packages(&quot;openintro&quot;) install.packages(&quot;GGally&quot;) install.packages(&quot;broom&quot;) library(tidyverse) library(openintro) library(GGally) library(broom) "],["exploratory-data-analysis.html", "2 Exploratory Data Analysis 2.1 Data", " 2 Exploratory Data Analysis 2.1 Data Many university courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, Beauty in the classroom: instructors' pulchritude and putative pedagogical productivity by Hamermesh and Parker found that instructors who are viewed to be better looking receive higher instructional ratings. In this lab we will analyze the data from this study in order to learn what goes into a positive lecturer evaluation. The data were gathered from end of semester student evaluations for a large sample of lecturers from the University of Texas at Austin. In addition, six students rated the lecturers' physical appearance. The result is a data frame where each row contains a different course/lecturer and columns represent variables about the courses and lecturers. The data frame is called evals and is contained in the openintro package (you can read more about it here). We can load the data and view the first few lines of the data by running the code below. data(&#39;evals&#39;, package=&#39;openintro&#39;) head(evals) It can be seen that the data contains 23 variables which we will explore to see if they have any relationship with the variable score, the evaluation score of each lecturer in each course (the \"response\" variable). These variables include both categorical and numerical types of variables. To find out more about each variable we can use the help function by running ?evals. 2.1.1 Exploring the data Before we start to model the data, we need to understand the data and the different types of variables. What type of study design is this? Experimental Observational 2.1.2 Exercise 1 Plot the score variable using a histogram to investigate its distribution. You can specify the colour of your plot by using the fill = ... argument (see here for some colours with their names in R). Hint ggplot(data = ???, aes(???)) + geom_histogram(???) Solution ggplot(data = evals, aes(x = score)) + geom_histogram(fill = &quot;violetred&quot;, bins = 40) Is the distribution skewed? No Yes, left-skewed Yes, right-skewed 2.1.3 Exercise 2 Create a boxplot of score against rank to explore any relationship between the professor evaluation score and rank of professor based on academic appointment categories applied in the U.S. and Canada. The rank variable can take three values, teaching, tenure track and tenure, where teaching professors are generally the most junior, and tenured professors are the most senior. Hint ggplot(data = ???, aes(???)) + geom_boxplot(???) + coord_flip() Solution ggplot(data = evals, aes(x = score, y = rank)) + geom_boxplot(fill = &quot;steelblue2&quot;) + coord_flip() What are the values that are used when creating a boxplot? (There are multiple correct answers) Mode 1st Quartile Variance Mean Maximum Standard Deviation 3rd Quartile Minimum Median Which of the ranks corrensponds to the highest average score? Tenure-track Tenured Teaching Which of the boxplots has the largest spread (based on the IQR)? Tenure-track Teaching Tenured Create a boxplot of score against ethnicity to explore any relationship, also using colours to distinguish between the different values of gender. Hint ggplot(data = ???, aes(???)) + geom_boxplot(???) Solution ggplot(data = evals, aes(x = ethnicity, y = score )) + geom_boxplot(aes(fill = gender)) Does it appear that different ethnic groups have a similar average score for both men and women? Yes No Does it appear there is a effect for gender within the ethnicity variable? Yes No Which boxplot would you think has the greatest variablility? Not Minority Ethnicity Male Not Minority Ethnicity Female Minority Ethnicity Female Minority Ethnicity Male "],["simple-linear-regression.html", "3 Simple Linear Regression 3.1 Exercise 3", " 3 Simple Linear Regression The fundamental phenomenon suggested by the study is that better looking teachers are evaluated more favourably. The variable bty_avg is the average beauty rating of lecturer (where 1 is the lowest and 10 is the highest). Let's create a scatterplot of these variables to see if there appears to be a relationship: ggplot(data = evals, aes(x = bty_avg, y = score)) + geom_point(color = &quot;salmon2&quot;) Before we draw any conclusions from the plot we can re plot using geom_jitter. ggplot(data = evals, aes(x = bty_avg, y = score)) + geom_jitter(color = &quot;skyblue2&quot;) geom_jitter adds a small amount of random variation to the location of each point, it is a useful way of handling overplotting caused by discreteness in the data. From the scatterplot there appears to be a (slight) positive relationship between evaluation score and average beauty. However, we can see if the apparent trend in the plot is something more than natural variation by fitting a linear model to predict average lecturer score by average beauty rating. Add the line of the best fit model to your plot using the following: ggplot(data = evals, aes(x = bty_avg, y = score)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;) geom_smooth aids in seeing patterns in the presence of overplotting. In the code above, it fits a linear model using method = \"lm\" and fits the line of best fit over the plot of bty_avg and score. The blue line is the model fitted. The shaded grey area around the line is called a \"confidence band\" and tells you about the variability you might expect in your predictions. To remove the confidence band, use se = FALSE. Does it appear that average beauty score is a predictor of lecture evaluation score? No Yes 3.1 Exercise 3 Produce residual plots to evaluate whether the conditions of least squares regression are reasonable. Hint Create a model object first, e.g. using model1 &lt;- lm(score ~ bty_avg, data = evals), then produce the plots. Hint 1 model1 &lt;- lm(???) ggplot(data = ???, aes(???)) + geom_point(color = &quot;seagreen2&quot;) + geom_hline(???) + xlab(???) + ylab(???) ggplot(data = ???, aes(???)) + geom_histogram(bins = 40, fill = &quot;dodgerblue2&quot;) + xlab(???) ggplot(data = ???, aes(???)) + stat_qq() + stat_qq_line(col = 2) Hint 2 model1 &lt;- lm(score ~ bty_avg, data = evals) ggplot(data = ???, aes(???)) + geom_point(color = &quot;seagreen2&quot;) + geom_hline(???) + xlab(&quot;Fitted values&quot;) + ylab(&quot;Residuals&quot;) ggplot(data = ???, aes(???)) + geom_histogram(bins = 40, fill = &quot;dodgerblue2&quot;) + xlab(&quot;Residuals&quot;) ggplot(data = ???, aes(???)) + stat_qq() + stat_qq_line(col = 2) Solution model1 &lt;- lm(score ~ bty_avg, data = evals) ggplot(data = model1, aes(x = .fitted, y = .resid)) + geom_point(color = &quot;seagreen2&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + xlab(&quot;Fitted values&quot;) + ylab(&quot;Residuals&quot;) ggplot(data = model1, aes(x = .resid)) + geom_histogram(bins = 40, fill = &quot;dodgerblue2&quot;) + xlab(&quot;Residuals&quot;) ggplot(data = model1, aes(sample = .resid)) + stat_qq() + stat_qq_line(col = 2) From the residuals, does it appear that the conditions of least squares regression are reasonable? Yes, since the residuals are scattered evenly above and below the zero line and while neither the histogram nor the QQ-plot are perfect, they both look adequate to show the residuals are approximately normally distributed. No, since the residuals aren't scattered evenly above and below the zero line, the histogram does not look bell shaped and centred at zero since and many of the QQ-plot points are very far from the line "],["collinearity.html", "4 Collinearity", " 4 Collinearity The data set contains several variables on the beauty score of the lecturer: individual ratings from each of the six students who were asked to score the physical appearance of the lecturers and the average of these six scores. Let's take a look at the relationship between one of these scores and the average beauty score. ggplot(data = evals, aes(x = bty_f1lower, y = bty_avg)) + geom_point(color = &quot;slateblue2&quot;) evals %&gt;% summarise(cor(bty_avg, bty_f1lower)) The variable bty_f1lower is the beauty ratings of the lecturers by the first female student from a lower level course. As expected, the relationship is quite strong---after all, the average score is calculated using the individual scores. We can look at the relationships between all beauty variables (columns 13 through 19) using the following command: evals %&gt;% select(contains(&quot;bty&quot;)) %&gt;% ggpairs(., lower = list(continuous = wrap(&quot;points&quot;, size=0.1))) These predictor variables are collinear (correlated), and adding more than one of these variables to the model would not add much value to the model. Review Section 9.1.2 OpenIntro to read more about collinearity OpenIntro Page 348. The plot shows the correlation between each pair of variables which also can be seen that each pair is over 0.5. In this application and with these highly-correlated predictors, it is reasonable to use the average beauty score as the single representative of these variables. By reducing the 7 columns to just 1 removes some correlation between the variables making it easier to investigate any relationships with our response score. "],["multiple-predictors.html", "5 Multiple Predictors 5.1 Exercise 4 5.2 Interpretation of a Categorical Variable with 2 levels 5.3 Interpretation of a Categorical Variable with more than 2 levels", " 5 Multiple Predictors In order to see if beauty is still a significant predictor of lecturer score after you've accounted for the lecturer's gender, you can add the gender term into the model. model1 &lt;- lm(score ~ bty_avg, data = evals) summary(model1) model2 &lt;- lm(score ~ bty_avg + gender, data = evals) summary(model2) 5.1 Exercise 4 Produce residual plots for model2 created above. Hint 1 model2 &lt;- lm(???) ggplot(data = ???, aes(???)) + geom_point(color = &quot;seagreen2&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + xlab(???) + ylab(???) ggplot(data = ???, aes(???)) + geom_histogram(bins = 40, fill = &quot;dodgerblue2&quot;) + xlab(???) ggplot(data = ???, aes(???)) + stat_qq() + stat_qq_line(col = 2) Hint 2 model2 &lt;- lm(score ~ bty_avg + gender, data = evals) ggplot(data = ???, aes(???)) + geom_point(color = &quot;seagreen2&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + xlab(&quot;Fitted values&quot;) + ylab(&quot;Residuals&quot;) ggplot(data = ???, aes(???)) + geom_histogram(bins = 40, fill = &quot;dodgerblue2&quot;) + xlab(&quot;Residuals&quot;) ggplot(data = ???, aes(???)) + stat_qq() + stat_qq_line(col = 2) Solution model2 &lt;- lm(score ~ bty_avg + gender, data = evals) ggplot(data = model2, aes(x = .fitted, y = .resid)) + geom_point(color = &quot;seagreen2&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + xlab(&quot;Fitted values&quot;) + ylab(&quot;Residuals&quot;) ggplot(data = model2, aes(x = .resid)) + geom_histogram(bins = 40, fill = &quot;dodgerblue2&quot;) + xlab(&quot;Residuals&quot;) ggplot(data = model2, aes(sample = .resid)) + stat_qq() + stat_qq_line(col = 2) From the residuals, does it appear that the conditions of least squares regression are reasonable? Yes, since the residuals are scattered evenly above and below the zero line and while neither the histogram nor the QQ-plot are perfect, they both look adequate to show the residuals are approximately normally distributed. No, since the residuals aren't scattered evenly above and below the zero line, the histogram does not look bell shaped and centred at zero since and many of the QQ-plot points are very far from the line Is bty_avg still a significant predictor of score? No Yes Has the addition of gender to the model changed the parameter estimate for bty_avg? Yes No Has the addition of gender to the model provided a better model fit? Yes, since the R-sqaured value has increased. No, since the R-squared value has decreased. Note the estimate for gender is now called gendermale in model2 summary above. We can see this name change whenever we introduce a categorical variable. The reason is that R re-codes gender from having the values of male and female to being an indicator variable called gendermale that takes a value of \\(0\\) for female lecturers and a value of \\(1\\) for male lecturers. (Such variables are often referred to as \"dummy\" variables). The fitted model (model2) is therefore: \\[ \\begin{aligned} \\widehat{score} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times bty\\_avg + \\hat{\\beta}_2 \\times gendermale \\\\ &amp;= 3.75 + 0.07 \\times bty\\_avg + 0.17 \\times gendermale \\end{aligned} \\] 5.2 Interpretation of a Categorical Variable with 2 levels The interpretation of parameters associated with categorical variables is slightly different than numerical variables. This is described within, OpenIntro Section 8.2.8 Page 323. The first two terms \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\times bty\\_avg\\) give the expected score for the first or baseline category or reference level (i.e. the category corresponding to an indicator value of 0), in this example female lecturers. The third term \\(\\hat{\\beta}_2 \\times gendermale\\) only appears for male lectures (i.e. when gendermale = 1) since the baseline is female, so for female lecturers, \\(\\hat{\\beta}_2\\) is multiplied by zero. \\(\\hat{\\beta}_2\\) is therefore the average change in the response variable between the two categories. So the estimated parameter value of 0.17239 from model2 above is the difference in the average evaluation score between male and female lecturers, with the average male's score being 0.17239 more than the average female score (when all other variables are held constant). The fitted model showing the relationships between score and bty_avg for the levels of gender can be shown by using the fitted values stored in model2 as follows: library(broom) model2 &lt;- lm(score ~ bty_avg + gender, data = evals) model2_aug &lt;- augment(model2) ggplot(data = model2_aug, mapping = aes(y = score, x = bty_avg, color = gender)) + geom_point(alpha = 0.3) + geom_line(aes(y = .fitted), lwd = 1) The augment function is part of the broom package and makes information from the fitted model more accessible. This plot shows why this model is referred to as a 'parallel lines model'. The decision to call the indicator variable gendermale instead of genderfemale has no deeper meaning. RStudio simply codes the category that comes first alphabetically as a \\(0\\). You can change the reference level of a categorical variable, which is the level that is coded as a 0, using therelevel() function. Use ?relevel to learn more. 5.2.1 Exercise 5 Create a new model called model3 with gender removed and rank added in. Then produce the model summary. Hint model3 &lt;- lm(???) summary(???) Solution model3 &lt;- lm(score ~ bty_avg + rank, data = evals) summary(model3) Note that the rank variable has three levels: teaching, tenure track, tenured (look back at the boxplots in Exercise 2 in the \"Exploratory Data Analysis\" section). 5.3 Interpretation of a Categorical Variable with more than 2 levels The interpretation of the coefficients associated with categorical variables in multiple regression is covered in OpenIntro Section 9.1.1 Page 344. When fitting a regression model with a categorical variable that has k levels where \\(k &gt; 2\\), R provides a coefficient for \\(k  1\\) of those levels. Therefore, one level does not receive a coefficient and this is the reference level or baseline. The coefficients listed for the other levels are all estimates of the difference between the respective levels to the reference level, while holding all other variables constant. The estimate for bty_avg estimates the change in expected evaluation score for every unit increase in beauty rating while holding all other variables constant. In this case, that translates into considering only lecturers of the same rank with bty_avg scores that are one point apart. The following plot shows the fitted model we called model3 (again note the 'parallel lines'). model3 &lt;- lm(score ~ bty_avg + rank, data = evals) model3_aug &lt;- augment(model3) ggplot(data = model3_aug, mapping = aes(y = score, x = bty_avg, color = rank)) + geom_point(alpha = 0.3) + geom_line(aes(y = .fitted), lwd = 1) "],["finding-the-best-model.html", "6 Finding the Best Model 6.1 Exercise 6 6.2 Exercise 7 6.3 Exercise 8 6.4 Exercise 9", " 6 Finding the Best Model Clearly in these data, as in many examples, there are many possible configurations of predictor variables. One approach to deciding which variables to include in our model is called backward elimination, described in OpenIntro Section 9.2 Page 353. We will start with a \"full\" model that predicts lecturer score based on rank, gender, ethnicity, language of the university where they got their degree, age, proportion of students that filled out evaluations, class size, course level, number of lecturers, number of credits, average beauty rating, outfit, and picture color. 6.1 Exercise 6 Create the full model explained above and name it fullmodel. Hint fullmodel &lt;- lm(??? ~ ??? + ... + ???, data = ???) summary(???) Solution fullmodel &lt;- lm(score ~ rank + gender + ethnicity + language + age + cls_perc_eval + cls_students + cls_level + cls_profs + cls_credits + bty_avg + pic_outfit + pic_color, data = evals) summary(fullmodel) What is the correct interpretation for the coefficient associated with the age variable? A one unit increase in age is associated with a 0.009 increase in score when all other variables are held constant. A one unit decrease in age is associated with a 0.009 increase in score. A one unit increase in age is associated with a 0.009 decrease in score when all other variables are held constant. A one unit decrease in age is associated with a 0.009 decrease in score when all other variables are held constant. What is the correct interpretation for the coefficient associated with the ethnicity variable? A slope of 0.12349 means that the model predicts a 0.123 unit higher score for those who are a minority ethnicity compared to those who are not a minority ethnicity when all other variables are held constant. A slope of 0.12349 means that the model predicts a 0.123 unit higher score for those who are not minority ethnicity compared to those who are a minority ethnicity when all other variables are held constant. A one unit increase in ethnicity is associated with a 0.12349 unit increase in score. An one unit increase in ethnicity is associated with a 0.12349 decrease in score. 6.2 Exercise 7 Drop the variable with the highest p-value and re-fit the model. Name this model m1. Produce a summary of the updated model. Hint m1 &lt;- lm(??? ~ ??? + ... + ???, data = ???) summary(???) Solution m1 &lt;- lm(score ~ rank + gender + ethnicity + language + age + cls_perc_eval + cls_students + cls_level + cls_credits + bty_avg + pic_outfit + pic_color, data = evals) summary(m1) Hopefully you saw that the highest p-value is associated with cls_profssingle - a categorical variable which states the number of lecturers teaching sections in course in sample, with the levels: single, multiple. Note that the coefficients and significance of the other predictor variables change when the variable cls_profs was removed. One of the things that makes multiple regression interesting is that coefficient estimates depend on the other variables that are included in the model. 6.3 Exercise 8 Run the following code to use backward elimination to find the 'best' model for the data. As we are using backward elimination, at each stage we drop the variable with the highest p-value which corresponds to the least significant variable to the model. m2 &lt;- lm(score ~ rank + gender + ethnicity + language + age + cls_perc_eval + cls_students + cls_credits + bty_avg + pic_outfit + pic_color, data = evals) summary(m2) m3 &lt;- lm(score ~ rank + gender + ethnicity + language + age + cls_perc_eval + cls_credits + bty_avg + pic_outfit + pic_color, data = evals) summary(m3) m4 &lt;- lm(score ~ gender + ethnicity + language + age + cls_perc_eval + cls_credits + bty_avg + pic_outfit + pic_color, data = evals) summary(m4) m5 &lt;- lm(score ~ gender + ethnicity + language + age + cls_perc_eval + cls_credits + bty_avg + pic_color, data = evals) summary(m5) The p-value for cls_students - the total number of students in the class - for the step when it was removed is (to 4 decimal places): Which variables have remained within the model? (Select and check multiple) age bty_avg pic_outfit cls_profs language gender pic_color cls_perc rank cls_level ethnicity cls_students cls_credits 6.4 Exercise 9 Produce residual plots for the final model produced. Run code to produce the model again before creating the residual plots and call the model finalmodel. Hint 1 finalmodel &lt;- lm(???) summary(???) ggplot(data = ???, aes(???)) + geom_point(color = &quot;seagreen2&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + xlab(???) + ylab(???) ggplot(data = ???, aes(???)) + geom_histogram(bins = 40, fill = &quot;dodgerblue2&quot;) + xlab(???) ggplot(data = ???, aes(???)) + stat_qq() + stat_qq_line(col = 2) Hint 2 finalmodel &lt;- lm(??? ~ ??? + ... + ???, data = ???) summary(???) ggplot(data = ???, aes(???)) + geom_point(color = &quot;seagreen2&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + xlab(&quot;Fitted values&quot;) + ylab(&quot;Residuals&quot;) ggplot(data = ???, aes(???)) + geom_histogram(bins = 40, fill = &quot;dodgerblue2&quot;) + xlab(&quot;Residuals&quot;) ggplot(data = ???, aes(???)) + stat_qq() + stat_qq_line(col = 2) Solution finalmodel &lt;- lm(score ~ gender + ethnicity + language + age + cls_perc_eval + cls_credits + bty_avg + pic_color, data = evals) summary(finalmodel) ggplot(data = finalmodel, aes(x = .fitted, y = .resid)) + geom_point(color = &quot;seagreen2&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + xlab(&quot;Fitted values&quot;) + ylab(&quot;Residuals&quot;) ggplot(data = finalmodel, aes(x = .resid)) + geom_histogram(bins = 40, fill = &quot;dodgerblue2&quot;) + xlab(&quot;Residuals&quot;) ggplot(data = finalmodel, aes(sample = .resid)) + stat_qq() + stat_qq_line(col = 2) Based on the final model, we can describe the characteristics of a lecturer and course at University of Texas at Austin that would be associated with a high evaluation score. The equation for this becomes: \\[ \\begin{aligned} \\text{score} &amp;= 3.7719 + 0.2071 \\text{ gender: male} + 0.1679 \\text{ ethnicity: not minority} \\\\ &amp; - 0.2062 \\text{ language non-english} - 0.0060 \\text{ age} + 0.0047 \\text{ Percent of students in class who completed evaluation} \\\\ &amp;+ 0.5053 \\text{ Number of credits of class: one credit} + 0.0511 \\text{ beauty average} \\\\ &amp; - 0.1906 \\text{ Colour of lecturer&#39;s picture: colour}. \\end{aligned} \\] Would you be comfortable generalising your conclusions to apply to lecturers generally (at any university)? Maybe No Yes Should the R squared or adjusted R squared be used for model selection in this example? R-squared Adjusted R-squared "],["group-exercise.html", "7 Group Exercise", " 7 Group Exercise Ensure that you have the following packages installed and loaded: install.packages(&quot;tidyverse&quot;) install.packages(&quot;openintro&quot;) install.packages(&quot;GGally&quot;) install.packages(&quot;broom&quot;) library(tidyverse) library(openintro) library(GGally) library(broom) Create a boxplot of language against score with colour for gender. Produce a pairs plot for the variables not included within the pairs plot produced in the lab and not including the ID variables (i.e not columns 1, 2 and 13 to 19). From the pairs plot produced in the lab and in Q2, determine which variables have the strongest correlation. Fit a multiple linear regression model for the relationship of score against bty_avg and age. Produce residual plots for the model above and determine if it provides a good fit. Compare this fit with the model fitted in the lab containing just bty_avg and gender. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
